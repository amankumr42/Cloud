# -*- coding: utf-8 -*-
"""ParseStorageDataToBigQuery.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sVGA27MiG2Ysu_vtRwVQNZC3b6fBNksA
"""

#!pip install apache-beam[gcp]

from __future__ import absolute_import
import apache_beam as beam
import argparse
import logging
import re

def run(argv=None):
  """Create data pipeline"""
  parser = argparse.ArgumentParser()
  parser.add_argument(
      '--output_table',
      required = True,
      help = (
          "Output table for BigQuery for results specified as:"
          "PROJECT:DATASET.TABLE or DATASET.TABLE."
      )
  )
  parser.add_argument(
      '--input_path',
      required = True,
      help = (
          "Input path of GS Storage"
          "Format -> gs://<path_to_bucket>/<folder>"
      )
  )
  known_args, pipeline_args = parser.parse_known_args(argv)

  with beam.Pipeline(argv=pipeline_args) as p:

    from apache_beam.io.gcp.internal.clients import bigquery

    table_schema = bigquery.TableSchema()

    # Create Schema for Biq Query Table

    host_schema =  bigquery.TableFieldSchema()
    host_schema.name = 'host'
    host_schema.type = 'string'
    host_schema.mode = 'nullable'
    table_schema.fields.append(host_schema)

    time_schema = bigquery.TableFieldSchema()
    time_schema.name = 'time'
    time_schema.type = 'string'
    time_schema.mode = 'nullable'
    table_schema.fields.append(time_schema)

    request_type_schema = bigquery.TableFieldSchema()
    request_type_schema.name = 'request'
    request_type_schema.type = 'string'
    request_type_schema.mode = 'nullable'
    table_schema.fields.append(request_type_schema)

    status_schema = bigquery.TableFieldSchema()
    status_schema.name = 'status'
    status_schema.type = 'string'
    status_schema.mode = 'nullable'
    table_schema.fields.append(status_schema)   

    size_schema = bigquery.TableFieldSchema()
    size_schema.name = 'size'
    size_schema.type = 'string'
    size_schema.mode = 'nullable'
    table_schema.fields.append(size_schema)
    
    HOST = r'^(?P<host>.*?)'
    SPACE = r'\s'
    IDENTITY = r'\S+'
    USER = r'\S+'
    TIME = r'(?P<time>\[.*?\])'
    REQUEST = r'\"(?P<request>.*?)\"'
    STATUS = r'(?P<status>\d{3})'
    SIZE = r'(?P<size>\S+)'
    REGEX = HOST+SPACE+IDENTITY+SPACE+USER+SPACE+TIME+SPACE+REQUEST+SPACE+STATUS+SPACE+SIZE+SPACE

    def parse_server_log(record):
      match = re.search(REGEX, record)
      return {
          'host' : match.group('host'),
          'time' : match.group('time'),
          'request' : match.group('request'),
          'status' : match.group('status'),
          'size' : match.group('size')
      }
    read_data = p | "Read from GCP Storage" >> beam.io.ReadFromText(known_args.input_path)
    parse_data = read_data | "Parse Log Data" >> beam.Map(parse_server_log)
    write_data = parse_data | "Write Data to BigQuery" >> beam.io.WriteToBigQuery(
        known_args.output_table,
        schema = table_schema,
        create_disposition = beam.io.BigQueryDisposition.CREATE_IF_NEEDED,
        write_disposition = beam.io.BigQueryDisposition.WRITE_APPEND
    )

if __name__ == '__main__' :    
  logging.getLogger().setLevel(logging.INFO)
  run()

'''
python ParseStorageDataToBigQuery.py  --input_path gs://<gs_storage>/access.log  --output project_123:apache_server_logs.apache_server_tbl  --runner DataflowRunner --project project123  --temp_location gs://<gs_storage>/temp1 --region us-east1
'''
